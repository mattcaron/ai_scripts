#!/bin/bash

# Run ollama with proper local arguments

# This is a separate file so we can just ssh in and run this one thing
# for easy remote use.

# Get our paths
source `dirname $0`/ai_paths

# export everything we're about to set because, from here on, it's
# configuration for what we are about to start.
set -a

# Ollama config

OLLAMA_HOST=127.0.0.1
OLLAMA_MODELS=${OLLAMA_BASE_DIR}/models
OLLAMA_KEEP_ALIVE=30m

# Required for ThunderAI to use local OLLAMA
OLLAMA_ORIGINS=moz-extension://*

# Start Ollama
${OLLAMA_BASE_DIR}/bin/ollama serve

